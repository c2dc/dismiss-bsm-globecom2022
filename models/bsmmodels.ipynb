{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OutxwC_tDs1O"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","import pandas as pd\n","import numpy as np\n","import time\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import precision_recall_curve, classification_report\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, auc\n","from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay, roc_curve\n","from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n","from itertools import cycle\n","import matplotlib.pyplot as plt\n","import joblib\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","\n","# training and test\n","windowList = ['02', '03', '08', '13', '18', '23']\n","modelMLList = ['dt', 'knn', 'rf', 'mlp', 'lstm']\n","featList = ['feat1', 'feat2', 'feat3']\n","labelList = ['multiclass', 'binary', 'atk_1', 'atk_2', 'atk_4', 'atk_8', 'atk_16']\n","np.random.shuffle(windowList)\n","np.random.shuffle(modelMLList)\n","np.random.shuffle(featList)\n","np.random.shuffle(labelList)\n","print(windowList)\n","print(modelMLList)\n","print(featList)\n","print(labelList)\n","for window in windowList:\n","  for modelML in modelMLList:\n","    simPath = \"/content/drive/MyDrive/dataset/veremi/veremiWithT2/simulationscsv\"\n","    globecomPath = \"/content/drive/MyDrive/globecom\"\n","    dismissPath = globecomPath+\"/dismiss\"\n","    windowPath = dismissPath+\"/\"+window+\"bsm\"\n","    preprocPath = windowPath+\"/preprocessing\"\n","    allMsgPath = preprocPath+\"/allmsg\"\n","    modelsPath = windowPath+\"/models\"\n","    modelPath = modelsPath+\"/\"+modelML\n","    resultsPath = dismissPath+\"/results\"\n","    paths = [simPath, globecomPath, dismissPath, windowPath, preprocPath, allMsgPath, modelsPath, modelPath, resultsPath]\n","    for p in paths:\n","      if os.path.exists(p) == False:\n","        os.mkdir(p)\n","    for feat in featList:\n","      for label in labelList:\n","       \n","        name = window + modelML + label + feat\n","        # build performance file\n","        if os.path.exists(resultsPath+'/performance.csv'):\n","          os.chdir(resultsPath)\n","          performance = pd.read_csv('performance.csv', index_col=0)\n","        else:\n","          dataPerformance = {}\n","          performance = pd.DataFrame.from_dict(dataPerformance, orient='index', columns=['precision', 'recall', 'f1score', 'accuracy'])\n","\n","        # build time of training file\n","        if os.path.exists(resultsPath+'/time.csv'):\n","          os.chdir(resultsPath)\n","          timeRecord = pd.read_csv('time.csv', index_col=0)\n","        else:\n","          dataTime = {}\n","          timeRecord = pd.DataFrame.from_dict(dataTime, orient='index', columns=['training', 'testing'])\n","        \n","        if (name not in performance.index.values) or (name not in timeRecord.index.values):\n","          display(performance)\n","          print(\"-\"*70)\n","          print(f'Processing {name}...')\n","\n","          # load the data\n","          if os.path.exists(allMsgPath+\"/allMsg.csv\"):\n","            os.chdir(allMsgPath)\n","            sample = pd.read_csv('allMsg.csv', index_col=0)\n","          else:\n","            os.chdir(preprocPath)\n","            simulations = pd.Series([f for f in os.listdir() if os.path.isfile(f)]).sort_values().reset_index(drop=True)[:]\n","            dflist = []\n","            for idx, item in enumerate(simulations):\n","              print('\\r', item, end='')\n","              df = pd.read_csv(item, index_col=0, header=0)\n","              dflist.append(df)\n","            sample = pd.concat(dflist).reset_index(drop=True)\n","            os.chdir(allMsgPath)\n","            sample.to_csv('allMsg.csv')\n","            del dflist, df\n","            # print('\\rSimulations pre-processed:', len(simulations))\n","            # print('All msg shape:', sample.shape)\n","          \n","          columns = []\n","          for column in sample.columns.values:\n","            if feat == 'feat1':\n","              if 'RSSI' in column:\n","                columns.append(column)\n","              elif 'distance' in column:\n","                columns.append(column)\n","            elif feat == 'feat2':\n","              if 'conformity' in column and '0' not in column:\n","                columns.append(column)\n","            elif feat == 'feat3':\n","              if 'RSSI' in column and '0' not in column:\n","                columns.append(column)\n","              elif 'distance' in column and '0' not in column:\n","                columns.append(column)\n","              elif 'conformity' in column and '0' not in column:\n","                columns.append(column)\n","          columns.append('attackerType')\n","        \n","          if label == 'multiclass':\n","            sample = sample[columns]\n","          elif label == 'binary':\n","            posLabel = 1\n","            sample = sample[columns]\n","            sample['attackerType'].loc[sample['attackerType'] != 0] = posLabel\n","          else:\n","            posLabel = int(label.split(\"_\")[1])\n","            sample = sample[columns]\n","            sample = sample.loc[(sample['attackerType'] == 0) | (sample['attackerType'] == posLabel)]\n","          # select the data\n","          data = sample.iloc[:, 0:-1].values\n","          classes = sample.iloc[:, -1].values\n","          # label binarize one-hot style\n","          lb = preprocessing.LabelBinarizer()\n","          lb.fit(classes)\n","          if modelML in ['mlp', 'lstm'] and label == 'multiclass':\n","            classes = lb.transform(classes)\n","          elif modelML in ['mlp', 'lstm']:\n","            classes = lb.transform(classes)\n","            classes = MultiLabelBinarizer().fit_transform(classes)\n","          data_train, data_test, classes_train, classes_test = train_test_split(data, classes, train_size=0.8, test_size=0.2, random_state=1)\n","          if os.path.exists(modelPath+\"/\"+label) == False:\n","            os.mkdir(modelPath+\"/\"+label)\n","          clf = []\n","          if modelML == 'dt':\n","            clf = DecisionTreeClassifier()\n","          elif modelML == 'knn':\n","            clf = KNeighborsClassifier()\n","          elif modelML == 'rf':\n","            clf = RandomForestClassifier()\n","          elif modelML == 'mlp':\n","            # modelo rede neural\n","            layer1 = keras.layers.Input(shape=(data_train.shape[1],))\n","            layer2 = keras.layers.Dense(7, activation=\"relu\")(layer1)\n","            layer3 = keras.layers.Dense(7, activation=\"relu\")(layer2)\n","            output = keras.layers.Dense(classes_train.shape[1], activation=\"softmax\")(layer3)\n","          elif modelML == 'lstm':\n","            # number of features per series\n","            if feat == 'feat1':\n","              serie = 2\n","            elif feat == 'feat2':\n","              serie = 1\n","            elif feat == 'feat3':\n","              serie = 3\n","            data_train=data_train.reshape(data_train.shape[0], int(data_train.shape[1]/serie), serie)\n","            data_test=data_test.reshape(data_test.shape[0], int(data_test.shape[1]/serie), serie)\n","            lstmunits = 32\n","            layer1 = keras.layers.Input(shape=(data_train.shape[1], data_train.shape[2]))\n","            layer2 = keras.layers.LSTM(lstmunits, return_sequences=True)(layer1)\n","            layer3 = keras.layers.LSTM(lstmunits)(layer2)\n","            output = keras.layers.Dense(classes_train.shape[1], activation=\"softmax\")(layer3)\n","\n","          if modelML in ['mlp', 'lstm']:\n","            clf = keras.Model(inputs=layer1, outputs=output, name=name)\n","            clf.compile(\n","                  loss=keras.losses.CategoricalCrossentropy(),\n","                  optimizer=keras.optimizers.Adam(),\n","                  metrics=[keras.metrics.Accuracy(),\n","                          keras.metrics.Recall(class_id=1)],\n","              )\n","            early_stopping = keras.callbacks.EarlyStopping(\n","                monitor=\"loss\",\n","                patience=3,\n","                min_delta=1e-4,\n","                restore_best_weights=True\n","            )\n","            # check condition for training\n","            while (os.path.exists(modelPath+\"/\"+label+\"/\"+feat+'/saved_model.pb') == False) or (name not in timeRecord.index.values):\n","              print('training', name)\n","              start_time = time.time()\n","              clf.fit(\n","                  data_train,\n","                  classes_train,\n","                  epochs=200,\n","                  batch_size=1000,\n","                  callbacks=[early_stopping]\n","              )\n","              trainingTime = time.time() - start_time\n","              clf.save(modelPath+\"/\"+label+\"/\"+feat)\n","              \n","              dataTime = {}\n","              dataTime[name] = [trainingTime, np.nan]\n","              dfTime = pd.DataFrame.from_dict(dataTime, orient='index', columns=['training', 'testing'])\n","              timeRecord.loc[name] = dfTime.loc[name]\n","              os.chdir(resultsPath)\n","              timeRecord.to_csv('time.csv')\n","              \n","              \n","          elif modelML in ['dt', 'knn', 'rf', 'svm']:\n","            # check condition for training\n","            while (os.path.exists(modelPath+\"/\"+label+\"/\"+name+'fit.pkl') == False) or (name not in timeRecord.index.values):\n","              print('training', name)\n","              start_time = time.time()\n","              clf.fit(data_train, classes_train)\n","              trainingTime = time.time() - start_time\n","              os.chdir(modelPath+\"/\"+label)\n","              joblib.dump(clf, name+'fit.pkl')\n","              \n","              \n","              dataTime = {}\n","              dataTime[name] = [trainingTime, np.nan]\n","              dfTime = pd.DataFrame.from_dict(dataTime, orient='index', columns=['training', 'testing'])\n","              timeRecord.loc[name] = dfTime.loc[name]\n","              os.chdir(resultsPath)\n","              timeRecord.to_csv('time.csv')            \n","              \n","              \n","\n","          os.chdir(resultsPath)\n","          timeRecord = pd.read_csv('time.csv', index_col=0)\n","          print('Training '+name+' done in', \"{:.2f}\".format(timeRecord['training'].loc[name])+' sec')\n","\n","          # test\n","          os.chdir(modelPath+\"/\"+label)\n","          clf = []\n","          if modelML in ['dt', 'knn', 'rf', 'svm']:\n","            clf = joblib.load(name+'fit.pkl' , mmap_mode ='r')\n","            proba = []\n","            start_time = time.time()\n","            proba = clf.predict_proba(data_test)\n","            predictTime = time.time() - start_time\n","            clTest = classes_test\n","          elif modelML in ['mlp', 'lstm']:\n","            clf = keras.models.load_model(modelPath+\"/\"+label+\"/\"+feat)\n","            proba = []\n","            start_time = time.time()\n","            proba = clf.predict(data_test)\n","            predictTime = time.time() - start_time\n","            clTest = lb.inverse_transform(classes_test)\n","          \n","          # log time of testing\n","          timeRecord['testing'].loc[name] = predictTime\n","          os.chdir(resultsPath)\n","          timeRecord.to_csv('time.csv')\n","          print('Predict '+name+' done in ', \"{:.2f}\".format(predictTime)+' sec')\n","          print(\"-\"*70)\n","\n","          if label == 'multiclass':\n","            # Classification Report\n","            pred = lb.inverse_transform(proba)\n","          else:\n","            # Best threshold\n","            precision, recall, thresholds = precision_recall_curve(clTest, proba[:, 1], pos_label=posLabel)\n","            # convert to f score\n","            np.seterr(divide='ignore', invalid='ignore')\n","            fscore = (2 * precision * recall) / (precision + recall)\n","            np.nan_to_num(fscore, copy=False)\n","            # locate the index of the largest f score\n","            ix = np.argmax(fscore)\n","            print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n","            print(\"-\"*70)\n","            # Classification Report\n","            pred = np.where(np.array(proba[:, 1]) >= thresholds[ix], posLabel, 0)\n","          classlist = []\n","          for cl in lb.classes_:\n","            classlist.append('class '+str(int(cl)))\n","          print('Classification Report for '+name)\n","          print(classification_report(clTest,\n","                                      pred,\n","                                      target_names=classlist,\n","                                      digits=3,\n","                                      zero_division=0))\n","          print(\"-\"*70)\n","          # Confusion matrix\n","          cm = confusion_matrix(clTest, pred, labels=lb.classes_)\n","          disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n","                                        display_labels=lb.classes_)\n","          disp.plot()\n","          plt.title(name)\n","          plt.savefig(name+'.pdf')\n","          plt.show()\n","          print(\"-\"*70)\n","\n","          if label == 'multiclass':\n","            # ROC CURVES\n","            if modelML in ['dt', 'knn', 'rf', 'svm']:\n","              lbclasses_test = lb.transform(classes_test)\n","            elif modelML in ['mlp', 'lstm']:\n","              lbclasses_test = classes_test\n","            n_classes = lbclasses_test.shape[1]\n","            # Compute ROC curve and ROC area for each class\n","            fpr = dict()\n","            tpr = dict()\n","            roc_auc = dict()\n","            for i in range(n_classes):\n","                fpr[i], tpr[i], _ = roc_curve(lbclasses_test[:, i], proba[:, i])\n","                roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","            # First aggregate all false positive rates\n","            all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n","\n","            # Then interpolate all ROC curves at this points\n","            mean_tpr = np.zeros_like(all_fpr)\n","            for i in range(n_classes):\n","                mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n","\n","            # Finally average it and compute AUC\n","            mean_tpr /= n_classes\n","\n","            fpr[\"macro\"] = all_fpr\n","            tpr[\"macro\"] = mean_tpr\n","            roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n","\n","            # Plot all ROC curves\n","            lw = 2\n","            plt.figure()\n","            # plt.plot(\n","            #     fpr[\"micro\"],\n","            #     tpr[\"micro\"],\n","            #     label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n","            #     color=\"deeppink\",\n","            #     linestyle=\":\",\n","            #     linewidth=4,\n","            # )\n","\n","            plt.plot(\n","                fpr[\"macro\"],\n","                tpr[\"macro\"],\n","                label=\"Macro Avg (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n","                color=\"navy\",\n","                linestyle=\":\",\n","                linewidth=4,\n","                alpha=0.5,\n","            )\n","\n","            colors = cycle([\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"])\n","            for i, color in zip(range(n_classes), colors):\n","                labelClasses = int(lb.classes_[i])\n","\n","                plt.plot(\n","                    fpr[i],\n","                    tpr[i],\n","                    color=color,\n","                    lw=lw,\n","                    label=\"Class {0} (area = {1:0.2f})\".format(labelClasses, roc_auc[i]),\n","                    alpha=0.5\n","                )\n","\n","            plt.plot([0, 1], [0, 1], \"k--\", lw=lw, color=\"grey\", alpha=0.2)\n","            plt.xlim([-0.02, 1.0])\n","            plt.ylim([0.0, 1.05])\n","            plt.xlabel(\"False Positive Rate\")\n","            plt.ylabel(\"True Positive Rate\")\n","            plt.legend(loc=\"lower right\")\n","            plt.title('ROCcurve '+name)\n","            os.chdir(modelPath+\"/\"+label)\n","            plt.savefig('ROCcurve '+name+'.pdf')\n","            plt.show()\n","            print(\"-\"*70)\n","\n","\n","            # PR CURVES\n","            # Compute PR curve and PR area for each class\n","            precision = dict()\n","            recall = dict()\n","            pr_auc = dict()\n","            # precision, recall, thresholds = precision_recall_curve(clTest, proba[:, 1], pos_label=posLabel)\n","            for i in range(n_classes):\n","                precision[i], recall[i], _ = precision_recall_curve(lbclasses_test[:, i], proba[:, i])\n","                pr_auc[i] = auc(recall[i], precision[i])\n","\n","            # First aggregate all false positive rates\n","            all_precision = np.unique(np.concatenate([precision[i] for i in range(n_classes)]))\n","\n","            # Then interpolate all pr curves at this points\n","            mean_recall = np.zeros_like(all_precision)\n","            for i in range(n_classes):\n","                mean_recall += np.interp(all_precision, precision[i], recall[i])\n","\n","            # Finally average it and compute AUC\n","            mean_recall /= n_classes\n","\n","            precision[\"macro\"] = all_precision\n","            recall[\"macro\"] = mean_recall\n","            pr_auc[\"macro\"] = auc(recall[\"macro\"], precision[\"macro\"])\n","\n","            # Plot all pr curves\n","            lw = 2\n","            plt.figure()\n","            # plt.plot(\n","            #     precision[\"micro\"],\n","            #     recall[\"micro\"],\n","            #     label=\"micro-average pr curve (area = {0:0.2f})\".format(pr_auc[\"micro\"]),\n","            #     color=\"deeppink\",\n","            #     linestyle=\":\",\n","            #     linewidth=4,\n","            # )\n","\n","            plt.plot(\n","                precision[\"macro\"],\n","                recall[\"macro\"],\n","                label=\"Macro Avg (area = {0:0.2f})\".format(pr_auc[\"macro\"]),\n","                color=\"navy\",\n","                linestyle=\":\",\n","                linewidth=4,\n","                alpha=0.5,\n","            )\n","\n","            colors = cycle([\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"])\n","            for i, color in zip(range(n_classes), colors):\n","                labelClasses = int(lb.classes_[i])\n","\n","                plt.plot(\n","                    precision[i],\n","                    recall[i],\n","                    color=color,\n","                    lw=lw,\n","                    label=\"Class {0} (area = {1:0.2f})\".format(labelClasses, pr_auc[i]),\n","                    alpha=0.5\n","                )\n","\n","            plt.xlim([0.15, 1.02])\n","            plt.ylim([-0.01, 1.05])\n","            plt.xlabel(\"False Positive Rate\")\n","            plt.ylabel(\"True Positive Rate\")\n","            plt.legend(loc=\"lower left\")\n","            plt.title('PRcurve '+name)\n","            os.chdir(modelPath+\"/\"+label)\n","            plt.savefig('PRcurve '+name+'.pdf')\n","            plt.show()\n","            print(\"-\"*70)\n","\n","          else:\n","            # Precision-Recall curve\n","            PrecisionRecallDisplay.from_predictions(clTest, proba[:, 1], pos_label=posLabel)\n","            plt.title('PR curve '+name)\n","            no_skill = len(clTest[clTest==1]) / len(clTest)\n","            plt.plot([0,1], [no_skill,no_skill], linestyle='--', color=\"grey\", label='No Skill')\n","            plt.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best threshold')\n","            plt.legend()\n","            plt.savefig('PRcurve '+name+'.pdf')\n","            plt.show()\n","            print(\"-\"*70)\n","            # ROC curve\n","            RocCurveDisplay.from_predictions(clTest, proba[:, 1], pos_label=posLabel)\n","            plt.title('ROC curve '+name)\n","            plt.plot([0, 1], [0, 1], color=\"grey\", lw=1, linestyle=\"--\")\n","            plt.savefig('ROCcurve '+name+'.pdf')\n","            plt.show()\n","            print(\"-\"*70)\n","\n","          # save the results to a csv\n","          if label == 'multiclass':\n","            prScore = precision_score(clTest, pred, average='macro', zero_division=0)\n","            rcScore = recall_score(clTest, pred, average='macro', zero_division=0)\n","            f1Score = f1_score(clTest, pred, average='macro', zero_division=0)\n","            accScore = accuracy_score(clTest, pred)\n","          else:\n","            prScore = precision_score(clTest, pred, pos_label=posLabel, zero_division=0)\n","            rcScore = recall_score(clTest, pred, pos_label=posLabel, zero_division=0)\n","            f1Score = f1_score(clTest, pred, pos_label=posLabel, zero_division=0)\n","            accScore = accuracy_score(clTest, pred)\n","          dataPerformance = {}\n","          dataPerformance[name] = [prScore, rcScore, f1Score, accScore]\n","\n","          dfPerformance = pd.DataFrame.from_dict(dataPerformance, orient='index', columns=['precision', 'recall', 'f1score', 'accuracy'])\n","\n","          os.chdir(resultsPath)\n","          try:\n","            performance = pd.read_csv('performance.csv', index_col=0)\n","          except FileNotFoundError:\n","            dfPerformance.to_csv('performance.csv')\n","            performance = dfPerformance\n","          else:\n","            try:\n","              performance.loc[name] = dfPerformance.loc[name]\n","            except KeyError:\n","              performance = pd.concat([performance, dfPerformance])\n","            performance.to_csv('performance.csv')\n","\n","print('performance.csv')\n","display(performance)\n","print(\"-\"*70)"]}],"metadata":{"colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"bsmmodels.ipynb","provenance":[],"authorship_tag":"ABX9TyMQ5E95Lxtt/xyp1K7M0IHX"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}